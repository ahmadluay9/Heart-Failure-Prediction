{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVoZ7li_kbW7"
      },
      "source": [
        "# Graded Challenge 3 Phase 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc7k2oL2kbW8"
      },
      "source": [
        "# Heart Failure Prediction based on Ensamble Classifier: Random Forest Classifier and Adaptive Boost Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmumbtgPkbW8"
      },
      "source": [
        "Ahmad Luay Adnani, Batch: FTDS-018-RMT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c57CW8oWkbW8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jkCpCfukbW9"
      },
      "source": [
        "# i. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TvyXz30kbW9"
      },
      "source": [
        "> This section contains the background analysis, problem statements and conceptual problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw7U70fskbW-"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5eX9vtykbW-"
      },
      "source": [
        "Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide. Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n",
        "\n",
        "Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n",
        "\n",
        "People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqjCKtEOkbW_"
      },
      "source": [
        "## Problem Statements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOql33LYkbW_"
      },
      "source": [
        "Since there are many features that can influence the mortaility by heart failure, this analysis will __determine whether a patient will develop  a heart failure or not during their follow up period__, because of the suspected variables.\n",
        "\n",
        "The analysis will use Ensamble Classifier: __Random Forest Classifier (RF)__ and __Adaptive Boost Classifier (AdaBoost)__. From those two algorithm, there will be an analysis for comparing the classification model and how it suggests the result of heart failure classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY4OV45NkbXA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmxWyKAYkbXA"
      },
      "source": [
        "# ii. Query SQL\n",
        "> This section contains SQL queries from Google Cloud Platform that were used in the data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtgCvNuekbXA"
      },
      "source": [
        "```\n",
        "SELECT * FROM `ftds-hacktiv8-project.phase1_ftds_018_rmt.heart-failure`\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KolKFP3_kbXB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0RoDZowkbXB"
      },
      "source": [
        "# iii. Importing Libraries\n",
        "> This section contains the process of importing libraries and checking the version of the libraries that will be used in this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIteByyKkbXB"
      },
      "source": [
        "## Import Libraries\n",
        "This section explains how to import the libraries that will be used in the analysis. The main library used for this analysis will be `scikit-learn`. In addition to scikit-learn, the libraries used in this analysis include `pandas`, `numpy`, `matplotlib`, and `seaborn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXm3WF6YkbXC",
        "outputId": "501d8fc0-c78f-4716-a762-8c534be072ab"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import imblearn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# handling outliers\n",
        "from feature_engine.outliers import Winsorizer\n",
        "\n",
        "# Split Dataset, Standarize, and Hyperparameter Tuning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.pipeline import Pipeline\n",
        "# from sklearn.pipeline import Pipeline\n",
        "\n",
        "# import Random Forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# handling imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Evaluate Classification Models\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Save and Load Model\n",
        "import pickle\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ_5w7ZgkbXD"
      },
      "source": [
        "## Libraries Version\n",
        "This section explains how to check the version of the libraries that will be used in the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbrNX7QzkbXE",
        "outputId": "3dde3008-db29-407d-e06d-c105054900d2"
      },
      "outputs": [],
      "source": [
        "# library version\n",
        "print('pandas version :',(pd.__version__))\n",
        "print('numpy version :',(np.__version__))\n",
        "print('matplotlib version :',(matplotlib.__version__))\n",
        "print('seaborn version :',(sns.__version__))\n",
        "print('scikit-learn version :',(sklearn.__version__))\n",
        "print('imbalanced-learn version :',(imblearn.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VySUjVwekbXF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toibDa35kbXF"
      },
      "source": [
        "# iv. Data Loading\n",
        "> This section explains the data preparation process prior to further data exploration. The data preparation process carried out in this section includes the process of data loading, checking the size of the dataset, displaying information of the dataset, checking for missing values, checking for duplicated entries, and measuring of central tendency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI40o8orkbXG"
      },
      "source": [
        "## Data Loading\n",
        "This section explains the process of data loading. Dataset used in this analysis is `heart-failure` from Google Cloud Platform database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "4Li8JwEWkbXG",
        "outputId": "2fe3bb6d-8577-4da8-9b07-4d72b6d7b19b"
      },
      "outputs": [],
      "source": [
        "# Data Loading\n",
        "df= pd.read_csv('Dataset.csv')\n",
        "# Displaying the top 10 rows of the dataset\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "rNx3DDSnkbXG",
        "outputId": "cf5f61a1-2913-43f9-e72c-2706acfea853"
      },
      "outputs": [],
      "source": [
        "# Displaying the bottom 10 rows of the dataset\n",
        "df.tail(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UvIgKzfkkbXG"
      },
      "source": [
        "Following are the variables and definitions of each column in the dataset.\n",
        "\n",
        "Variable | Definition\n",
        "---|---\n",
        "`age` | Age in years\n",
        "`anemia` | Decrease of red blood cells or hemoglobin (0 = no anaemia, 1 = has anaemia)\n",
        "`creatinine_phosphokinase` | Level of the CPK enzyme in the blood (mcg/L)\n",
        "`diabetes` | If the patient has diabetes (0 = no diabetes, 1 = has diabetes)\n",
        "`ejection_fraction` | Percentage of blood leaving the heart at each contraction (percentage)\n",
        "`high_blood_pressure` | If the patient has hypertension (0 = no, 1 = yes)\n",
        "`platelets` | Platelets in the blood (kiloplatelets/mL)\n",
        "`serum_creatinine` | \tLevel of serum creatinine in the blood (mg/dL)\n",
        "`serum_sodium` | Level of serum sodium in the blood (mEq/L)\n",
        "`sex` | Woman or man (Female = 0, Male = 1)\n",
        "`smoking` | If the patient smokes or not (0 = no, 1 = yes)\n",
        "`time` | Follow-up period (days)\n",
        "`DEATH_EVENT` | \tIf the patient deceased during the follow-up period (0 = no, 1= yes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdEDz7DFkbXH"
      },
      "source": [
        "## Size of Dataset\n",
        "This section explains how to check the size of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDlEldiAkbXH",
        "outputId": "d1c0b462-2acd-4168-bf54-284f07aa8de7"
      },
      "outputs": [],
      "source": [
        "# size of dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6yOIDMSkbXH"
      },
      "source": [
        "## Information\n",
        "This section explains the information contained in the dataset, including the index, columns, data type, non-null values, and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMwEwwWckbXH",
        "outputId": "0d7f6aff-1d43-404c-f9ff-c1a1f6b20251"
      },
      "outputs": [],
      "source": [
        "# showing information from the dataset\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Owv8bkikbXH",
        "outputId": "9674c3b3-5639-4e1b-f013-beb3175babd2"
      },
      "outputs": [],
      "source": [
        "# convert dtype\n",
        "df = df.astype({'age':'float','anaemia':'float','creatinine_phosphokinase':'float','diabetes':'float','ejection_fraction':'float',\n",
        "                'high_blood_pressure':'float','platelets':'float','serum_creatinine':'float','serum_sodium':'float','sex':'float',\n",
        "                'smoking':'float','time':'float','DEATH_EVENT':'float'})\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r49qSe8kbXI"
      },
      "source": [
        "## Missing Values\n",
        "This section explains the process for checking missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-itgvzekbXI",
        "outputId": "97e56e4f-c518-4073-f8ad-3106de1df87b"
      },
      "outputs": [],
      "source": [
        "# check for missing value\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z28Z4zOjkbXI"
      },
      "source": [
        "There is no missing value in this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrt3DwVckbXI"
      },
      "source": [
        "## Checking for Duplicated Data\n",
        "This section explains the process to check for duplicated data in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8swUpv4kbXJ",
        "outputId": "d451b396-fe3c-495d-ff6a-b76ac6be266f"
      },
      "outputs": [],
      "source": [
        "# Check for duplicated entries\n",
        "df.duplicated().value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1vVQEyzkbXJ"
      },
      "source": [
        "There is no duplicared data in this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCa2hwdgkbXJ"
      },
      "source": [
        "## Measure of Central Tendency\n",
        "The following are the measure of central tendency of the dataset used in this analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "l9QmP7NEkbXJ",
        "outputId": "98f3154f-df21-4736-f8e8-e67659f6468a"
      },
      "outputs": [],
      "source": [
        "# measure of central tendency\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-lpBOPhkbXJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teyYuchLkbXK"
      },
      "source": [
        "# v. Exploratory Data Analysis\n",
        "> This section contains data exploration of the dataset used in this analysis. Two exploratory data analyses are carried out: one to find general information and another to analyze the correlation matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EheLoKPkbXK"
      },
      "source": [
        "## Numerical and Categorical Columns\n",
        "This section describes the process for classifying numerical and categorical data in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBpm6OAtkbXK",
        "outputId": "7e0f18f1-b65d-4008-de57-093ba0c97154"
      },
      "outputs": [],
      "source": [
        "# Unique Value counts of all columns\n",
        "df_copy =df.copy()\n",
        "for i in df_copy:\n",
        "    print(i,':')\n",
        "    print()\n",
        "    print(df_copy[i].unique())\n",
        "    print('-' * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzzAz0OKkbXK",
        "outputId": "5fc8be3d-d700-4bf6-afe3-8fc33d125b5c"
      },
      "outputs": [],
      "source": [
        "# separating numerical and categorical columns\n",
        "numerical = ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']\n",
        "print('Numerical Column :',numerical)\n",
        "categorical = ['anaemia','diabetes','high_blood_pressure','sex','smoking','DEATH_EVENT']\n",
        "print('Categorical Column :',categorical)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu4FvqjckbXL"
      },
      "source": [
        "## Number of Death Event\n",
        "This section describes data exploration to find out the number of `DEATH_EVENT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "KpopiVHHkbXL",
        "outputId": "5a962069-0932-491b-d643-22afdae86dbd"
      },
      "outputs": [],
      "source": [
        "# death_event  value count\n",
        "death_event = df.DEATH_EVENT.value_counts().to_frame()\n",
        "death_event = death_event.reset_index()\n",
        "death_event['index'] = death_event['index'].replace({0:'No',1:'Yes'})\n",
        "death_event\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create pie chart\n",
        "labels = 'No', 'Yes'\n",
        "colors = sns.color_palette('deep')\n",
        "explode = (0.05, 0)\n",
        "\n",
        "plt.pie(df_copy.DEATH_EVENT.value_counts(),explode=explode,labels=labels, colors = colors, autopct='%.0f%%',shadow=True,startangle=60)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.title('Death Event')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8JeqJYbkbXL"
      },
      "source": [
        "Number of deaths after the following days are different, where __Non-Death are 36% greater than Death__. This will be keep in mind if there is any imbalance data or not. But first, the death_event--as the target--will be compared with other variables so we can get the conclusion for the skewness and handling imbalance data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuePl6uGkbXL"
      },
      "source": [
        "## Gender Distribution\n",
        "This section describes data exploration to find out the number of `DEATH_EVENT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "ihH1vw3QkbXL",
        "outputId": "c5800af2-5c7b-4329-d8a7-2e7ee2b074b6"
      },
      "outputs": [],
      "source": [
        "# gender value count\n",
        "sex = df.groupby(by=['sex','DEATH_EVENT']).aggregate(Number_of_DEATH_EVENT=('DEATH_EVENT','count'))\n",
        "sex = sex.reset_index()\n",
        "sex['sex'] = sex['sex'].replace({0:'Female',1:'Male'})\n",
        "sex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace values in the DEATH_EVENT column\n",
        "df_copy[\"DEATH_EVENT\"] = df_copy[\"DEATH_EVENT\"].replace({0: \"No\", 1: \"Yes\"})\n",
        "\n",
        "# create bar chart\n",
        "fig, ax = plt.subplots(figsize=(20, 8))\n",
        "\n",
        "sns.countplot(x=df_copy.DEATH_EVENT, hue=df_copy.sex,palette='winter',order=df_copy.DEATH_EVENT.value_counts().index)\n",
        "\n",
        "plt.title('Death Event by Gender')\n",
        "plt.xlabel('Death Event')\n",
        "plt.ylabel('Number of Death  Event')\n",
        "plt.legend(loc='upper right',labels=['Female','Male'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azChZAWFkbXM"
      },
      "source": [
        "From the table and visualization above, it can be seen that the number of male patients with heart failure is more than female patients. Where about 32% die during the follow-up period. Further data exploration is necessary to find out the condition of male patients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUqjx7qzkbXM"
      },
      "source": [
        "### Male Patients Condition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYbPeU2AkbXM"
      },
      "source": [
        "This section describes data exploration to find out what conditions most often cause patients to die during the follow-up period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "-xBCRJbHkbXM",
        "outputId": "04fb00db-1547-4e8d-b589-7fc711cd8db6"
      },
      "outputs": [],
      "source": [
        "# male patients deceased during the follow up period \n",
        "df_male = df_copy.loc[(df_copy['sex']==1)&\n",
        "                    (df_copy['DEATH_EVENT']==\"Yes\")]\n",
        "df_male.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "E6EJuaAfkbXN",
        "outputId": "fc5ba52b-6504-4e14-bbaa-74034d8f5461"
      },
      "outputs": [],
      "source": [
        "# Replace values in the anaemia column\n",
        "df_male[\"anaemia\"] = df_male[\"anaemia\"].replace({0: \"No\", 1: \"Yes\"})\n",
        "# anemia value counts\n",
        "df_male.groupby(by=['anaemia','DEATH_EVENT']).aggregate(Number_of_DEATH_EVENT=('DEATH_EVENT','count'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "tVusD-AukbXN",
        "outputId": "8ca348f6-41c1-45d8-c138-b709e046cbb2"
      },
      "outputs": [],
      "source": [
        "# Replace values in the diabetes column\n",
        "df_male[\"diabetes\"] = df_male[\"diabetes\"].replace({0: \"No\", 1: \"Yes\"})\n",
        "# diabetes value counts\n",
        "df_male.groupby(by=['diabetes','DEATH_EVENT']).aggregate(Number_of_DEATH_EVENT=('DEATH_EVENT','count'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "hrK1YP0vkbXN",
        "outputId": "be7ead42-632c-4b4d-b903-c44f7465b2d4"
      },
      "outputs": [],
      "source": [
        "# Replace values in the high_blood_pressure column\n",
        "df_male[\"high_blood_pressure\"] = df_male[\"high_blood_pressure\"].replace({0: \"No\", 1: \"Yes\"})\n",
        "# high_blood_pressure value counts\n",
        "df_male.groupby(by=['high_blood_pressure','DEATH_EVENT']).aggregate(Number_of_DEATH_EVENT=('DEATH_EVENT','count'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "W2_x2uFgkbXN",
        "outputId": "b5d5fbe9-72f9-4692-abc6-2a0c9ea622c5"
      },
      "outputs": [],
      "source": [
        "# Replace values in the smoking column\n",
        "df_male[\"smoking\"] = df_male[\"smoking\"].replace({0: \"No\", 1: \"Yes\"})\n",
        "# smoking value counts\n",
        "df_male.groupby(by=['smoking','DEATH_EVENT']).aggregate(Number_of_DEATH_EVENT=('DEATH_EVENT','count'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "SacLZf1NkbXO",
        "outputId": "6e9f17b6-5f73-4b24-d283-091760055d0a"
      },
      "outputs": [],
      "source": [
        "# Create Bar Charts\n",
        "sns.set(font_scale=2)\n",
        "fig, ax = plt.subplots(1,4, sharex=True, figsize=(40,25))\n",
        "sns.countplot(ax=ax[0],x=df_male['anaemia'], palette='winter')\n",
        "ax[0].set_title('Male patients with anemia')\n",
        "sns.countplot(ax=ax[1],x=df_male['diabetes'], palette='winter')\n",
        "ax[1].set_title('Male patients with diabetes')\n",
        "sns.countplot(ax=ax[2],x=df_male['high_blood_pressure'], palette='winter')\n",
        "ax[2].set_title('Male patients with high blood pressure')\n",
        "sns.countplot(ax=ax[3],x=df_male['smoking'], palette='winter')\n",
        "ax[3].set_title('Male patients with habit of smoking')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f25uieoHkbXO"
      },
      "source": [
        "From the table and visualization above, Male patients who have __smoking habits__ have a higher chance of dying during follow up periods than any other conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA6bH4fbkbXO"
      },
      "source": [
        "## Comparison between Death Event with other variables\n",
        "This section describes data exploration to compare the number of patients who are still alive and died during the follow-up period with other variables in this dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "Q1p_yzUCkbXO",
        "outputId": "ad876ad2-7271-4454-e703-ecb3375c0369"
      },
      "outputs": [],
      "source": [
        "# Creating new dataframe for the histogram\n",
        "sns.set(font_scale=1)\n",
        "output = 'DEATH_EVENT'\n",
        "cols = [f for f in df.columns if df.dtypes[f] != \"object\"]\n",
        "f = pd.melt(df, id_vars=output, value_vars=cols)\n",
        "\n",
        "# Creating histogram\n",
        "g = sns.FacetGrid(f, hue=output, col=\"variable\", col_wrap=4, sharex=False, sharey=False )\n",
        "g = g.map(sns.histplot, \"value\", kde=True).add_legend()\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv2vuTdskbXO"
      },
      "source": [
        "Based on the histogram above, we can see that the distribution of __Not Death__ is still dominating that Death. However, we should check wherer variable time looks different than the others, where Death is high with time between 0-100 days. From here we should check the skewness of time as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fJcoppOkbXP"
      },
      "source": [
        "## Correlation Matrix Analysis\n",
        "This section explains about correlation matrix analysis to find out the correlation between features and target (`DEATH_EVENT`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k4kN1uakbXP"
      },
      "source": [
        "The cell below explains the process of performing a correlation matrix analysis to identify the features that are most strongly correlated with the target (`DEATH_EVENT`). To accomplish this, categorical data will be converted into numerical data using the `LabelEncoder` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "X4du9CdXkbXP",
        "outputId": "51be1d5c-d139-408e-d4fa-14efbf82daab"
      },
      "outputs": [],
      "source": [
        "# Using LabelEncoder to convert categorical into numerical data\n",
        "m_LabelEncoder = LabelEncoder()\n",
        "\n",
        "for col in df_copy[categorical]:\n",
        "    df_copy[col]=m_LabelEncoder.fit_transform(df_copy[col])\n",
        "df_copy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "Bsz786Y7kbXP",
        "outputId": "3c9d2e96-868a-497b-eece-d4adec35dc15"
      },
      "outputs": [],
      "source": [
        "# Plotting Correlation Matrix of Features and DEATH_EVENT\n",
        "sns.set(font_scale=1)\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(df_copy.corr(),annot=True,cmap='coolwarm', fmt='.2f')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yTaHtmRkbXP",
        "outputId": "3dd32f4a-0abd-474a-f0d8-320a1168802d"
      },
      "outputs": [],
      "source": [
        "# Get List of All Correlation\n",
        "\n",
        "final_column = df.corr()['DEATH_EVENT'] >= 0.05\n",
        "final_column = final_column[final_column==True]\n",
        "final_column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQj72iEHkbXQ",
        "outputId": "0b8b889b-7678-4409-e4db-6883d3b3d4d9"
      },
      "outputs": [],
      "source": [
        "# Get List of All Correlation\n",
        "\n",
        "final_column2 = df.corr()['DEATH_EVENT'] <= -0.05\n",
        "final_column2 = final_column2[final_column2==True]\n",
        "final_column2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf-H5bX9kbXQ"
      },
      "source": [
        "Based on visualization above, the `education_level`, `sex`, `marital_status` has a low correlation to the target (`DEATH_EVENT`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sugPOnSikbXQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vBuwZT5kbXQ"
      },
      "source": [
        "# vi. Feature Engineering / Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYb0M80ukbXQ"
      },
      "source": [
        "> This section explains the process of preparing data for the model training process, such as data cleaning, creating data inference, separating data into train-tests, data transformation (normalization, encoding, etc.), and other processes needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrzEkmUskbXQ"
      },
      "source": [
        "## Data Inference\n",
        "This section explains how to create a data inference. Data inference is distinct from the train and test set, as it is still based on the original dataset and used to evaluate the model's performance on new, raw data. To create the data inference, we will set aside 10 data from the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "tOZGkDGfkbXR",
        "outputId": "159c7bf1-85dc-4437-e6d0-06101a4ccd29"
      },
      "outputs": [],
      "source": [
        "# Creating data inference\n",
        "data = df.copy()\n",
        "df_inf = data.sample(10, random_state=0).sort_index()\n",
        "print('size of data inference (rows,columns) : ',df_inf.shape)\n",
        "df_inf.reset_index(drop=True).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gHdNtjrkbXR"
      },
      "outputs": [],
      "source": [
        "# Saving data inference as .csv file\n",
        "df_inf.to_csv('inference.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "0D8pqXjEkbXR",
        "outputId": "5dd1e494-4b0e-4503-d6e4-d3a218f0f3f5"
      },
      "outputs": [],
      "source": [
        "# Removing inference from dataset \n",
        "df_train_test = data.drop(df_inf.index)\n",
        "print('size of original dataset (rows,columns) :',data.shape)\n",
        "print('size of dataset after data inference is removed (rows,columns) :',df_train_test.shape)\n",
        "df_train_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64K5wlXCkbXR"
      },
      "source": [
        "## Separating Dataset into Training Set and Testing Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1SND5aCkbXR"
      },
      "source": [
        "This section explains how to separate dataset into training set and testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "VL4PBtuDkbXR",
        "outputId": "802db4df-afea-42d0-f977-466474964148"
      },
      "outputs": [],
      "source": [
        "# Defining X and y (removing unnecesary features)\n",
        "X = df_train_test.drop(['DEATH_EVENT'],axis=1)\n",
        "y = pd.DataFrame(df_train_test['DEATH_EVENT'])\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7NUm9QFkbXS"
      },
      "source": [
        "## Separating Dataset into Training Set and Testing Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wUJfWV1kbXS"
      },
      "source": [
        "This section explains how to separate dataset into training set and testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "vRSKJK1mkbXS",
        "outputId": "54f3469f-f592-4095-f991-88a2f7a1b8b2"
      },
      "outputs": [],
      "source": [
        "# Separating Dataset into Training Set and Testing Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,  # features\n",
        "                                                   y, # target = death event\n",
        "                                                   test_size=0.3, # test_set 30%\n",
        "                                                   random_state=0) \n",
        "\n",
        "print('Train Size: ', X_train.shape)\n",
        "print('Test Size: ', X_test.shape)\n",
        "X_train = X_train.sort_index()\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "0uQX2J4pkbXS",
        "outputId": "bfb4f5bc-57f8-4604-e073-e559e2d96d54"
      },
      "outputs": [],
      "source": [
        "# X_test set\n",
        "X_test = X_test.sort_index()\n",
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "804yqeV_kbXS",
        "outputId": "93abe2ae-b1fe-4cea-ad8a-5c4dd0026791"
      },
      "outputs": [],
      "source": [
        "# y_train set\n",
        "y_train = y_train.sort_index()\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "YT2kByf9kbXT",
        "outputId": "6114fd28-fa2f-4610-d5c8-bcf6b55880b5"
      },
      "outputs": [],
      "source": [
        "# y_test set\n",
        "y_test = y_test.sort_index()\n",
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rj01Dd0kbXT"
      },
      "source": [
        "## Check the distribution of data\n",
        "This section explains how to check the distribution of the data. The histogram and boxplot visualizations below show the distribution of numerical data in the train set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtjiZ9iskbXT"
      },
      "outputs": [],
      "source": [
        "# numerical columns\n",
        "num_col = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g61ouSNEkbXT",
        "outputId": "46e1f5a4-bdb4-467f-e7de-dc183f42cb2d"
      },
      "outputs": [],
      "source": [
        "# plot histogram and boxplot\n",
        "# numerical columns\n",
        "num = X_train[num_col]\n",
        "n=len(num.columns)\n",
        "sns.set(font_scale=2)\n",
        "fig, ax = plt.subplots(n,2,figsize=(30,68))\n",
        "for i in range(n):\n",
        "    col = num.columns[i]\n",
        "    skewness = X_train[col].skew()\n",
        "    sns.histplot(ax=ax[i,0],data=X_train[col])\n",
        "    ax[i, 0].set_title(f'{col} skewness: {skewness:.3f}')\n",
        "    sns.boxplot(ax=ax[i,1],data=X_train,x=X_train[col],width=0.50)\n",
        "    ax[i,1].set_title(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3bZ7YZekbXT"
      },
      "outputs": [],
      "source": [
        "# Create Function to calculate skewness\n",
        "def skewness(df,col):\n",
        "  skewness = df[col].skew()\n",
        "  if skewness>=-0.5 and skewness<=0.5:\n",
        "    print(f'Disribution of {col} : Normal Distribution')\n",
        "  else:\n",
        "    print(f'Disribution of {col} : Skewed Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kMh5hf2kbXU",
        "outputId": "09f17c14-a161-46d7-e04e-1cda2d3c8786"
      },
      "outputs": [],
      "source": [
        "# Distribution\n",
        "num = X_train[num_col]\n",
        "n=len(num.columns)\n",
        "\n",
        "for i in range(n):\n",
        "    col = num.columns[i]\n",
        "    skewness(X_train,col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82flC0UKkbXU"
      },
      "source": [
        "### Handling Outliers\n",
        "This section explains how to handle outliers in the train and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t-12MRMkbXU"
      },
      "outputs": [],
      "source": [
        "# Create Function\n",
        "def outlier_analysis(df,col):\n",
        "  skewness = df[col].skew()\n",
        "  if skewness>=-0.5 and skewness<=0.5:\n",
        "    upper = df[col].mean() + 3*X_train[col].std()\n",
        "    lower = df[col].mean() - 3*X_train[col].std()\n",
        "  else:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    upper = Q3 + (1.5 * IQR)\n",
        "    lower = Q1 - (1.5 * IQR)\n",
        "    \n",
        "  no_outliers = df[(df[col]>=lower) &  (df[col]<=upper)]\n",
        "  outliers = df[(df[col]>upper ) | (df[col]<lower) ]\n",
        "\n",
        "  return outliers,no_outliers, upper, lower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozUi4QZpkbXU"
      },
      "source": [
        "#### Handling Outliers in Train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs-Faj7TkbXU",
        "outputId": "a5aa133e-7789-4c46-b55b-15c70d490c9c"
      },
      "outputs": [],
      "source": [
        "# percentage of outlier in train set\n",
        "num = X_train[num_col]\n",
        "n=len(num.columns)\n",
        "\n",
        "for i in range(n):\n",
        "    col = num.columns[i]\n",
        "    out,no_out, up, low = outlier_analysis (X_train,col)\n",
        "    print(f'column name: {col}')\n",
        "    print('count of outlier: ', len(out))\n",
        "    print('percentage of outlier: ', (len(out)/len(X_train))*100, '%') \n",
        "    print('-'*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF9FFxMnkbXV"
      },
      "source": [
        "For columns with an outlier percentage less than 1%, handling outliers will be done using trimming, the rest will be handled using capping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR05mENNkbXV",
        "outputId": "f76faaf6-eab7-4760-86f3-6ce45e90a2a9"
      },
      "outputs": [],
      "source": [
        "# handling outlier in train set\n",
        "X_train_trimmed = X_train.copy()\n",
        "y_train_trimmed = y_train.copy()\n",
        "\n",
        "num = X_train[num_col]\n",
        "n=len(num.columns)\n",
        "for i in range(n):\n",
        "  col = num.columns[i]\n",
        "  out,no_out, up, low = outlier_analysis(X_train_trimmed,col)\n",
        "  pct_out = (len(out)/len(X_train))*100\n",
        "\n",
        "  if pct_out <= 1: # outliers below 1% will be removed\n",
        "    X_train_trimmed = X_train_trimmed[(X_train_trimmed[col]>=low) & (X_train_trimmed[col]<=up)]\n",
        "    y_train_trimmed.drop(index = out.index,inplace=True)\n",
        "\n",
        "print(f\"Size of X_train set - before outlier handling: {len(X_train)}\") \n",
        "print(f\"Size of X_train set - after outlier handling: {len(X_train_trimmed)}\")\n",
        "print(f\"Size of y_train set - before outlier handling: {len(y_train)}\") \n",
        "print(f\"Size of y_train set - after outlier handling: {len(y_train_trimmed)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mFLkgG_kbXV",
        "outputId": "5e20e94c-e7ad-47bf-f4e9-5c864aee4fe6"
      },
      "outputs": [],
      "source": [
        "# capping outlier if exist and in large proportion\n",
        "wins = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=num_col)\n",
        "X_train_capped = wins.fit_transform(X_train_trimmed)\n",
        "print('Size of dataset - After Capped  : ', X_train_capped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H67mLujHkbXV",
        "outputId": "2fb3ab38-c68e-4cd5-859a-e1a2cff0136d"
      },
      "outputs": [],
      "source": [
        "# Boxplot visualization\n",
        "num = X_train[num_col]\n",
        "n=len(num.columns)\n",
        "sns.set(font_scale=2)\n",
        "fig, ax = plt.subplots(n,2,figsize=(30,68))\n",
        "for i in range(n):\n",
        "    col = num.columns[i]\n",
        "    sns.boxplot(ax=ax[i,0],data=X_train,x=X_train[col],width=0.50)\n",
        "    ax[i,0].set_title(f'{col} in train set')\n",
        "    sns.boxplot(ax=ax[i,1],data=X_train_capped,x=X_train_capped[col],width=0.50)\n",
        "    ax[i,1].set_title(f'{col} in train set - after outlier handling')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5--Z9mqkbXW"
      },
      "source": [
        "## Feature Selection\n",
        "This section explains the process of creating a model with feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzYO9l0JkbXX",
        "outputId": "388adf51-3c85-44c0-f0ca-76bf2c3e2429"
      },
      "outputs": [],
      "source": [
        "# check feature importances\n",
        "#create classifier with n estimator = 100\n",
        "featimpt = RandomForestClassifier(n_estimators= 100, random_state=15)\n",
        "\n",
        "#fit to the data\n",
        "featimpt.fit(X_train_capped,y_train_trimmed)\n",
        "\n",
        "#simpan hasil dan buat menjadi dataframe\n",
        "scorepercolumn = pd.Series(featimpt.feature_importances_, index = X_train.columns)\n",
        "scorepercolumn.sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5BFJQn8kbXX"
      },
      "source": [
        "Because all categorical features have low scores, these features will not be used in modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "sdSSlOopkbXX",
        "outputId": "ce4befed-8f0a-47c6-d8e8-edc9ecc166c9"
      },
      "outputs": [],
      "source": [
        "# Drop Unnecessary Features\n",
        "Drop_Columns = ['sex','high_blood_pressure','diabetes','smoking','anaemia']\n",
        "X_train_final = X_train_capped.drop(Drop_Columns,axis=1).sort_index()\n",
        "X_test_final = X_test.drop(Drop_Columns,axis=1).sort_index()\n",
        "y_train_final= y_train_trimmed\n",
        "y_test_final= y_test\n",
        "X_train_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "417TtUP-kbXX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yICKSVzNkbXX"
      },
      "source": [
        "# vii. Model Definition\n",
        "> This section contains cells to define the model. The algorithms that will be used are :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY7fqZElkbXY"
      },
      "source": [
        "## Check whether the dataset is imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y47T50lvkbXY",
        "outputId": "82182064-4b8e-4ce9-a828-5fe8108de171"
      },
      "outputs": [],
      "source": [
        "# check target in train set\n",
        "y_train_final.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu8okokskbXY"
      },
      "source": [
        "Because the dataset is imbalance, the imbalance handling is carried out using __SMOTE__ from the `imbalanced-learn` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI2Bq3xUkbXY"
      },
      "outputs": [],
      "source": [
        "# handling imbalance with SMOTE\n",
        "smote = SMOTE(random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7m45TdxkbXY"
      },
      "source": [
        "## Creating Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zvrC2HckbXY"
      },
      "source": [
        "This section contains about the creation of pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd6Jd_DTkbXZ"
      },
      "outputs": [],
      "source": [
        "# numerical and categorical columns\n",
        "num_skewcols = ['creatinine_phosphokinase','ejection_fraction', 'platelets', 'serum_sodium','serum_creatinine']\n",
        "num_nomcols = ['age','time']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa1SXaaZkbXZ"
      },
      "outputs": [],
      "source": [
        "# Using ColumnTransformer for feature scaling\n",
        "preprocess = ColumnTransformer([\n",
        "    ('numskew', MinMaxScaler(),num_skewcols),\n",
        "    ('numnom', StandardScaler(),num_nomcols)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EVwYy4LkbXZ"
      },
      "outputs": [],
      "source": [
        "# creating pipeline for RandomForestClassifier\n",
        "prepmod_RF = Pipeline([\n",
        "    ('preprop', preprocess),\n",
        "    ('smote',smote),\n",
        "    ('RFC', RandomForestClassifier(random_state=0))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnFQHSAEkbXZ"
      },
      "outputs": [],
      "source": [
        "# creating pipeline for AdaBoostClassifier\n",
        "prepmod_AB = Pipeline([\n",
        "    ('preprop', preprocess),\n",
        "    ('smote',smote),\n",
        "    ('AB', AdaBoostClassifier())\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zq6_bOCkbXZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA9XlvWukbXZ"
      },
      "source": [
        "# viii. Model Training and Evaluation\n",
        "> This section explains the process to train the model that we defined using the algorithm that has been determined in the previous chapter. Those algorithms will be checked its accuracy through cross-validation from the train dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I0NF80okbXZ"
      },
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7NsICHOkbXa"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFhovleokbXa",
        "outputId": "4ed39d12-f34c-4968-bf56-16bdc8d98a98"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "# Cross Validation using `cross_val_score` for train set\n",
        "cv_rf = cross_val_score(prepmod_RF, \n",
        "                                  X_train_final, \n",
        "                                  y_train_final, \n",
        "                                  cv=5, \n",
        "                                  scoring='f1')\n",
        "\n",
        "print(\"f1 Score - All - Cross Validation  :\", cv_rf)\n",
        "print(\"f1 Score - Mean - Cross-Validation :\", cv_rf.mean())\n",
        "print(\"f1 Score - Std - Cross-Validation  :\", cv_rf.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AUxfKrRkbXg"
      },
      "source": [
        "### AdaBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMvp2YbPkbXh",
        "outputId": "bf5b9cfb-b2a0-411d-f999-6af75ade2822"
      },
      "outputs": [],
      "source": [
        "# AdaBoost\n",
        "# Cross Validation using `cross_val_score` for train set\n",
        "cv_ada = cross_val_score(prepmod_AB, \n",
        "                                  X_train_final, \n",
        "                                  y_train_final, \n",
        "                                  cv=5, \n",
        "                                  scoring='f1')\n",
        "\n",
        "print(\"f1 Score - All - Cross Validation  :\", cv_ada)\n",
        "print(\"f1 Score - Mean - Cross-Validation :\", cv_ada.mean())\n",
        "print(\"f1 Score - Std - Cross-Validation  :\", cv_ada.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Comparision based on Cross-Validation Score\n",
        "crossval = [cv_rf,cv_ada]\n",
        "mean = []\n",
        "std = []\n",
        "for i in crossval:\n",
        "    mean.append(i.mean())\n",
        "    std.append(i.std())\n",
        "model = ['Random Forest','AdaBoost']\n",
        "bestmodel = pd.DataFrame(model)\n",
        "bestmodel['Mean']=np.round(mean,3)\n",
        "bestmodel['Std']=np.round(std,3)\n",
        "bestmodel.rename(columns={0:'Algorithm Model'},inplace = True)\n",
        "bestmodel.sort_values(by= 'Mean' , ascending = False).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the cross-validation score, the **AdaBoost** algorithm has the highest mean score (0.613) and the lowest standard deviation score (0.067). Next, model evaluation is conducted using this algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Model - Baseline Parameter\n",
        "best_model = prepmod_AB\n",
        "\n",
        "# Fitting AdaBoost Classifier model - Baseline Parameter\n",
        "best_model.fit(X_train_final,y_train_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Model Evaluation  - Baseline Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model evaluation is conducted using the selected algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia6uKBigkbXh"
      },
      "outputs": [],
      "source": [
        "# Predicting target (y_train and y_test) with AdaBoost\n",
        "y_pred_train_baseline  = best_model.predict(X_train_final)\n",
        "y_pred_test_baseline  = best_model.predict(X_test_final)\n",
        "\n",
        "# Creating target prediction dataframe\n",
        "y_pred_train_baseline  = pd.DataFrame(data = y_pred_train_baseline , columns = [\"Prediction Train\"])\n",
        "y_pred_test_baseline  = pd.DataFrame(data = y_pred_test_baseline , columns = [\"Prediction Test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzFzuLQWkbXh"
      },
      "source": [
        "##### Classification Report & Confusion Matrix\n",
        "- __Train Set__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "xhE6ZvcWkbXh",
        "outputId": "dcd048b0-c2c2-4c54-91a1-79633914abb1"
      },
      "outputs": [],
      "source": [
        "# Determining classification report for train set\n",
        "print('---------Classification Report (Train Set)---------')\n",
        "print(classification_report(y_train_final,y_pred_train_baseline))\n",
        "\n",
        "# Creating confusion matrix for train set\n",
        "sns.set(font_scale=1)\n",
        "cm = confusion_matrix(y_train_final, y_pred_train_baseline, labels=best_model.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=best_model.classes_)\n",
        "\n",
        "disp.plot()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4DaEoPTkbXi"
      },
      "source": [
        "- __Test Set__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "mweKmjQwkbXi",
        "outputId": "5c0eed94-3cac-40c3-a802-5fb50185ee84"
      },
      "outputs": [],
      "source": [
        "# Determining classification report for test set\n",
        "print('---------Classification Report (Test Set)---------')\n",
        "print(classification_report(y_test_final,y_pred_test_baseline))\n",
        "\n",
        "\n",
        "# Creating confusion matrix for test set\n",
        "sns.set(font_scale=1)\n",
        "cm = confusion_matrix(y_test_final, y_pred_test_baseline, labels=best_model.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=best_model.classes_)\n",
        "\n",
        "disp.plot()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# # Save Classification Report into a Dictionary\n",
        "# score_reports_ada = {\n",
        "#     'train - precision' : precision_score(y_train_final, y_pred_train_baseline),\n",
        "#     'train - recall' : recall_score(y_train_final, y_pred_train_baseline),\n",
        "#     'train - accuracy' : accuracy_score(y_train_final, y_pred_train_baseline),\n",
        "#     'train - f1_score' : f1_score(y_train_final, y_pred_train_baseline),\n",
        "#     'test - precision' : precision_score(y_test_final, y_pred_test_baseline),\n",
        "#     'test - recall' : recall_score(y_test_final, y_pred_test_baseline),\n",
        "#     'test - accuracy' : accuracy_score(y_test_final, y_pred_test_baseline),\n",
        "#     'test - f1_score' : f1_score(y_test_final, y_pred_test_baseline),\n",
        "# }\n",
        "# all_reports['Baseline_adaboost'] = score_reports_ada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The baseline model seems to perform well overall. On the test set, the precision score is  0.826, which means it's making fewer correct positive predictions. The recall on the test set is 0.594, indicating that it's not identifying as many positive cases as in the training set. Despite this, the test accuracy is still decent at 0.805. The F1 score of 0.691 on the test set also reflects the trade-off between precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogkQRezckbXi"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVi7niZtkbXj"
      },
      "outputs": [],
      "source": [
        "# Best Model\n",
        "# create parameter grid\n",
        "param_grid = [{'AB__n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20],\n",
        "               'AB__learning_rate': [(0.97 + x / 100) for x in range(0, 8)],\n",
        "               'AB__algorithm': ['SAMME', 'SAMME.R'],\n",
        "               }]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G2PmZJnvkbXj",
        "outputId": "5723a282-1c61-4128-b818-0632583465da"
      },
      "outputs": [],
      "source": [
        "# Train with Grid Search\n",
        "grid_search  = GridSearchCV(best_model, param_grid = param_grid,refit = True,verbose=3)\n",
        "\n",
        "# Fit the Grid Search on the training data\n",
        "grid_search.fit(X_train_final, y_train_final)\n",
        "\n",
        "# Get the best estimator from the Grid Search\n",
        "best_est = grid_search.best_estimator_\n",
        "\n",
        "# Get the best hyperparameters found during the Grid Search\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation  - GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "8gW4F3dMkbXj",
        "outputId": "a5e10a04-7f60-4eed-ed69-e005da528466"
      },
      "outputs": [],
      "source": [
        "# Check Performance Model against Train-Set\n",
        "\n",
        "y_pred_train_grid  = best_est.predict(X_train_final)\n",
        "y_pred_test_grid  = best_est.predict(X_test_final)\n",
        "\n",
        "# Creating confusion matrix for train set\n",
        "print('Classification Report : \\n', classification_report(y_train_final, y_pred_train_grid ))\n",
        "print('Confusion Matrix Train Set')\n",
        "cm = confusion_matrix(y_train_final, y_pred_train_grid , labels=best_est.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=best_est.classes_)\n",
        "\n",
        "disp.plot()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Jn89Pf3RkbXk",
        "outputId": "e7de178d-fe8d-4d71-9303-8d73e004bd0b"
      },
      "outputs": [],
      "source": [
        "# Creating confusion matrix for test set\n",
        "\n",
        "print('Classification Report : \\n', classification_report(y_test_final, y_pred_test_grid))\n",
        "print('Confusion Matrix Test Set')\n",
        "cm = confusion_matrix(y_test_final, y_pred_test_grid, labels=best_est.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=best_est.classes_)\n",
        "\n",
        "disp.plot()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Classification Report into a Dictionary\n",
        "all_reports_hyper = {}\n",
        "score_reports_baseline = {\n",
        "    'train - precision' : round(precision_score(y_train_final, y_pred_train_baseline),3),\n",
        "    'train - recall' : round(recall_score(y_train_final, y_pred_train_baseline),3),\n",
        "    'train - accuracy' : round(accuracy_score(y_train_final, y_pred_train_baseline),3),\n",
        "    'train - f1_score' : round(f1_score(y_train_final, y_pred_train_baseline),3),\n",
        "    'test - precision' : round(precision_score(y_test_final, y_pred_test_baseline),3),\n",
        "    'test - recall' : round(recall_score(y_test_final, y_pred_test_baseline),3),\n",
        "    'test - accuracy' : round(accuracy_score(y_test_final, y_pred_test_baseline),3),\n",
        "    'test - f1_score' : round(f1_score(y_test_final, y_pred_test_baseline),3),\n",
        "}\n",
        "all_reports_hyper['baseline'] = score_reports_baseline\n",
        "\n",
        "score_reports_grid = {\n",
        "    'train - precision' : round(precision_score(y_train_final, y_pred_train_grid),3),\n",
        "    'train - recall' : round(recall_score(y_train_final, y_pred_train_grid),3),\n",
        "    'train - accuracy' : round(accuracy_score(y_train_final, y_pred_train_grid),3),\n",
        "    'train - f1_score' : round(f1_score(y_train_final, y_pred_train_grid),3),\n",
        "    'test - precision' : round(precision_score(y_test_final, y_pred_test_grid),3),\n",
        "    'test - recall' : round(recall_score(y_test_final, y_pred_test_grid),3),\n",
        "    'test - accuracy' : round(accuracy_score(y_test_final, y_pred_test_grid),3),\n",
        "    'test - f1_score' : round(f1_score(y_test_final, y_pred_test_grid),3),\n",
        "}\n",
        "all_reports_hyper['GridSearchCV'] = score_reports_grid\n",
        "\n",
        "# create dataframe\n",
        "pd.DataFrame(all_reports_hyper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on these results, the **GridSearchCV** model has slightly higher accuracy, precision, recall, and F1-score on the testing set, indicating better overall performance compared to the baseline model. Therefore, the **GridSearchCV** model is considered better in this comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best model\n",
        "best_model = best_est"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can gather the following insights from model evaluation: The GridSearchCV model outperforms the baseline in most aspects, including precision, accuracy, and F1 score on the test set.\n",
        "-  **Precision:**\n",
        "\n",
        "For the \"Not Dead\" class (0.0), the precision is 0.84. This means that when the model predicts an individual as \"Not Dead,\" it's correct about 84% of the time.\n",
        "For the \"Dead\" class (1.0), the precision is 0.96. This indicates that when the model predicts an individual as \"Dead,\" it's correct about 96% of the time.\n",
        "- **Recall (Sensitivity):**\n",
        "\n",
        "For the \"Not Dead\" class (0.0), the recall is 0.98. This suggests that the model is able to identify about 98% of the actual \"Not Dead\" cases.\n",
        "For the \"Dead\" class (1.0), the recall is 0.69. This means the model identifies around 69% of the actual \"Dead\" cases.\n",
        "- **F1-Score:**\n",
        "\n",
        "The F1-score is a balanced measure of precision and recall. For the \"Not Dead\" class (0.0), the F1-score is 0.91, indicating a good balance between precision and recall for this class.\n",
        "For the \"Dead\" class (1.0), the F1-score is 0.80, reflecting the balance between precision and recall for this class.\n",
        "- **Accuracy:**\n",
        "\n",
        "The overall accuracy of the model is 0.87, indicating that it correctly predicts the outcome for approximately 87% of the cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s378WjFnkbXl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGCeLS_akbXm"
      },
      "source": [
        "# ix. Model Saving\n",
        "> This section explains the process of saving files related to the results of the models that we created in the previous chapter. Since there are only 10 data, handling outliers could be skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m58XQmPCkbXm"
      },
      "outputs": [],
      "source": [
        "# Saving Model Files\n",
        "\n",
        "with open('best_model.pkl', 'wb') as file_1:\n",
        "  pickle.dump(best_model, file_1)\n",
        "\n",
        "with open('Drop_Columns.txt', 'w') as file_2:\n",
        "  json.dump(Drop_Columns, file_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this project, model inference is performed in a different notebook.\n",
        "\n",
        "Model Inference Notebook: https://github.com/ahmadluay9/Credit-Risk-Prediction/blob/main/Inference_credit_risk_prediction.ipynb\n",
        "\n",
        "Summary\n",
        "The credit risk predictions made by the model seem to align well with the actual credit risk status for the majority of the data points, indicating a reasonable performance of the classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH3iQ3OZogAp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rqHSQZHogyK"
      },
      "source": [
        "# x. Conceptual Problems\n",
        "> This section answers the conceptual problems:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i3qIlnlYqdFm"
      },
      "source": [
        "1. Jelaskan latar belakang adanya bagging dan cara kerja bagging !\n",
        "\n",
        "  Bagging (from bootstrap aggregating), is a a method that can improve the results of a machine learning classification algorithm by combining classification predictions from several models. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
        "\n",
        "2. Jelaskan perbedaan cara kerja algoritma Random Forest dengan algoritma boosting yang Anda pilih !\n",
        "  - Data Sampling Technique\n",
        "\n",
        "      In **Random forest**, the training data is sampled based on the bagging technique.\n",
        "\n",
        "      **Adaptive Boost Classifier** is based on boosting technique.\n",
        "      \n",
        "      [Boosting]('https://www.analyticsvidhya.com/blog/2021/04/best-boosting-algorithm-in-machine-learning-in-2021/') can be referred to as a set of algorithms whose primary function is to convert weak learners to strong learners. They have become mainstream in the Data Science industry because they have been around in the machine learning community for years. Boosting was first introduced by Freund and Schapire in the year 1997 with their AdaBoost algorithm, and since then, Boosting has been a prevalent technique for solving binary classification problems.\n",
        "\n",
        "  - Estimate Calculation\n",
        "\n",
        "      **Random Forest** aims to decrease variance not bias.\n",
        "\n",
        "      **Adaptive Boost Classifier** aims to decrease bias, not variance.\n",
        "      \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-5kxRDWt9EU"
      },
      "source": [
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXxNS1Gt-6m"
      },
      "source": [
        "# xi. Conclusion and Recommendation\n",
        "> This section contains the conclusions of the analysis that has been carried out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zjdoMrBuhGd"
      },
      "source": [
        "1. Based on Exploratory Data Analysis:\n",
        "  - Number of deaths after the following days are different, where __Non-Death are 36% greater than Death__.\n",
        "  - The number of male patients with heart failure is more than female patients. Where about 32% die during the follow-up period.\n",
        "  - Male patients who have smoking habits have a higher chance of dying during follow up periods than any other conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdJ4yJk8yghZ"
      },
      "source": [
        "2. Based on Model Evaluation:\n",
        "  - Based on the model evaluation, the score of the RandomForest algorithm with Hyperparameter Tuning is slightly greater than the score other algorithms.\n",
        "  - Can be seen from the `accuracy` score, this model is accurate enough to classify whether the patient died during the follow-up period or not and this model is good-fit.\n",
        "  - `f1_score`,`precision` and `recall` also quite good (>50%). The misclassification that occurs in this model is quite small, so that the doctor who treats the patient can supervise the patient properly during the follow-up period. But still need special attention, because there is still the possibility of misclassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v3VM6LTyhFB"
      },
      "source": [
        "3. Further Improvement:\n",
        "  - We can try to make a model with another algorithm model (Gradient Boosting, XGBoost in), to be compared with the currently used algorithm."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
